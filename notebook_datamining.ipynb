{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd55681",
   "metadata": {
    "id": "1fd55681"
   },
   "source": [
    "@Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164648c",
   "metadata": {
    "id": "d164648c"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this first task, you will **examine all data attributes and identify issues present in the data**. For each of the issues that you have identified, choose and perform necessary actions to address it. \n",
    "\n",
    "Finally, you will need to suitably split the data into two sets: one for training and one for testing, the latter contains 100 samples with missing class labels. Your marks for this task will depend on how well you identify the issues and address them. Below is a list of data preparation issues that you need to address\n",
    "\n",
    "    • Identify and remove irrelevant attributes.\n",
    "        - Att09 has a lot of missing values so maybe remove it\n",
    "    • Detect and handle missing entries\n",
    "        - Att00 has been replaced by the mean value for now\n",
    "    • Detect and handle duplicates (both instances and attributes).\n",
    "    • Select suitable data types for attributes.\n",
    "    • Perform data transformation (such as scaling/standardization) if needed.\n",
    "    • Perform other data preparation operations (This is optional, bonus marks will be awarded for novel ideas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b0ec5",
   "metadata": {
    "id": "639b0ec5"
   },
   "outputs": [],
   "source": [
    "DATA = 'https://github.com/kituyiharry/ClassData/raw/master/Assignment2021.sqlite'\n",
    "FNAME= 'Assignment2021.sqlite'\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "print(\"================== Fetching data\")\n",
    "\n",
    "urllib.request.urlretrieve(DATA, FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09d512",
   "metadata": {
    "id": "1f09d512"
   },
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b446443",
   "metadata": {
    "id": "0b446443"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c8dcf",
   "metadata": {
    "id": "e96c8dcf"
   },
   "source": [
    "Connect to the database via SQLIte and load into pandas dataframe\n",
    "* There are 1200 entries, the last 200 are missing attributes\n",
    "* _class_ column is what we are trying to predict\n",
    "* Some attributes are numeric, some categorical\n",
    "\n",
    "From Assignment description:\n",
    "The data is known to contain imperfections:\n",
    " * There are missing/corrupted entries in the data set.\n",
    " * There are duplicates, both instances and attributes.\n",
    " * There are irrelevant attributes that do not contain any useful information useful for the classification task.\n",
    " * The labelled data is imbalanced: there is a considerable difference between the number of samples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9b0f0",
   "metadata": {
    "id": "00b9b0f0"
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"Assignment2021.sqlite\")\n",
    "df  = pd.read_sql(\"SELECT * from data;\", con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d961aa0",
   "metadata": {
    "id": "5d961aa0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351635ae",
   "metadata": {
    "id": "351635ae"
   },
   "outputs": [],
   "source": [
    "df[:1000].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8ce4c",
   "metadata": {
    "id": "47d8ce4c"
   },
   "outputs": [],
   "source": [
    "# pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "#NB: Att01 is categorical\n",
    "df[:1000][['Att0'+str(n) for n in range(0,10) ]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb997c3",
   "metadata": {
    "id": "5cb997c3"
   },
   "source": [
    "Check column by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798808a6",
   "metadata": {
    "id": "798808a6"
   },
   "outputs": [],
   "source": [
    "df[:1000][['Att1'+str(n) for n in range(0,10) ]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad7680",
   "metadata": {
    "id": "48ad7680"
   },
   "outputs": [],
   "source": [
    "df[:1000][['Att2'+str(n) for n in range(0,9) ]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2dbe9",
   "metadata": {
    "id": "bce2dbe9"
   },
   "source": [
    "Att09 has a lot of missing entries lets confirm\n",
    "\n",
    "Lets filter attributes with missing attributes, Display percentage of entries missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a489968",
   "metadata": {
    "id": "3a489968",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(filter(lambda f: f[1] > 0, map(lambda g: (g, (sum(df[:1000][g].isna())/len(df[:1000][g])) * 100 ) , df.columns[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b492577",
   "metadata": {
    "id": "7b492577"
   },
   "source": [
    "Around 60% is empty, Replace with mean value for the <1% Att00 for now. ~> 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd0219",
   "metadata": {
    "id": "e0fd0219"
   },
   "outputs": [],
   "source": [
    "list(map(lambda v: df[v].fillna(value=df[v].mean(), inplace=True), ['Att00', 'Att09']))\n",
    "\n",
    "df.drop(columns=['Att09'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76e21a",
   "metadata": {
    "id": "2d76e21a"
   },
   "outputs": [],
   "source": [
    "# See all types:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0249ade",
   "metadata": {
    "id": "f0249ade"
   },
   "source": [
    "Attributes 1, 8 and 29 have Object types and need further inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9a509",
   "metadata": {
    "id": "c8e9a509"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93496",
   "metadata": {
    "id": "d8d93496"
   },
   "outputs": [],
   "source": [
    "# Look at the distributions\n",
    "df[:1000].hist(grid=True,figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef07af7",
   "metadata": {
    "id": "5ef07af7"
   },
   "outputs": [],
   "source": [
    "# Overview of the Data\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "# Generator to create distinct items:\n",
    "def uniq(iterable):\n",
    "    seen = set()\n",
    "    for x in iterable:\n",
    "        if x in seen:\n",
    "            continue\n",
    "        seen.add(x)\n",
    "        yield x\n",
    "\n",
    "# NB: Consume all iterators to avoid problems\n",
    "\n",
    "uniforms    = ['Att{}'.format(str(n).zfill(2)) for n in [ 13,20,24]]\n",
    "categorical = ['Att01', 'Att08', 'Att29'] # Possibly multimodal\n",
    "bimodal     = ['Att21', 'Att23']          # possibly also categorical \n",
    "target      = ['class']\n",
    "irrelevant  = ['index']\n",
    "sparse      = ['Att09']\n",
    "gaussians   = list(filter(\n",
    "    lambda x: x not in uniq(chain(bimodal, uniforms, target, categorical, sparse, irrelevant)), \n",
    "    ['Att{}'.format(str(n).zfill(2)) for n in range(0,30)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kz4tcvnj591Q",
   "metadata": {
    "id": "Kz4tcvnj591Q"
   },
   "outputs": [],
   "source": [
    "df[:1000].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf384201",
   "metadata": {
    "id": "bf384201",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some of the Object types\n",
    "# Just for browsing some of the columns (Object types)\n",
    "\n",
    "print(\"============= Categorical columns\")\n",
    "\n",
    "setA01 = set(df['Att01'])        # ->. {'ACKH', 'BYUB', 'GHKA', 'LLTF', 'LWYW', 'OSUG', 'SCIJ', 'UJJW', 'UKEV'}\n",
    "setA08 = set(df['Att08'])        # ->  {'HFTX', 'YIFL'}\n",
    "setA29 = set(df['Att29'])        # ->  {'FLJD', 'HUUV', 'OELG', 'OQDJ', 'PJIY', 'TOYT', 'YLWZ'}\n",
    "\n",
    "pprint({\n",
    "    'A01': setA01,\n",
    "    'A08': setA08,\n",
    "    'A29': setA29\n",
    "})\n",
    "\n",
    "# Binarization of these attributes before deciding what to do with them\n",
    "# Convert them into Categorical types and replace them for feature engineering\n",
    "wascategorical = ['Att01', 'Att08', 'Att29'] # Possibly multimodal\n",
    "\n",
    "# For each object attribute, convert to a Categorical attribute\n",
    "cats = list(map(lambda c: df[c].astype('category'), wascategorical))\n",
    "\n",
    "\n",
    "df['Att01'] = cats[0] # For att01 \n",
    "df['Att08'] = cats[1] # For att08\n",
    "df['Att29'] = cats[2] # For att29 \n",
    "\n",
    "df[['Att01','Att08','Att29']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zcszWachYx2",
   "metadata": {
    "id": "1zcszWachYx2"
   },
   "source": [
    "Define a function to encode columns using an encoder into values we can use in our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93505708",
   "metadata": {
    "id": "93505708"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Assumption being the information is somehow conveyed in the ordering of the name \n",
    "# therefore AAAA < ZZZZ or Vice Versa\n",
    "\n",
    "def encodecategories(adf, column, encoder):\n",
    "    encoder.fit(adf[[column]])\n",
    "    tle = encoder.transform(adf[[column]])\n",
    "    adf[column] = tle\n",
    "    return encoder\n",
    "\n",
    "# Encode categorical entries\n",
    "\n",
    "att01enc = encodecategories(df, 'Att01', preprocessing.OrdinalEncoder())\n",
    "att08enc = encodecategories(df, 'Att08', preprocessing.OrdinalEncoder())\n",
    "att29enc = encodecategories(df, 'Att29', preprocessing.OrdinalEncoder()) \n",
    "\n",
    "df[categorical]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69tL7EIiude",
   "metadata": {
    "id": "a69tL7EIiude"
   },
   "source": [
    "Bin our numerical features to reduce small observation errors. This can help the classifiers better grok the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ff09e",
   "metadata": {
    "id": "9c4ff09e"
   },
   "outputs": [],
   "source": [
    "# Convert all categorical attributes to numerical \n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "# https://stackoverflow.com/a/32011969\n",
    "\n",
    "# Bin remaining columns and go categorical\n",
    "# Settling at 10 Bins for each of the remaining categories\n",
    "\n",
    "\n",
    "le,_ = df[:1000].shape\n",
    "# TAX = 5\n",
    "# NUMBINS = int(sqrt(le)) - TAX # https://stackoverflow.com/a/33458879\n",
    "NUMBINS = int(1 + (3.322 * log(le))) # Sturges Rule\n",
    "\n",
    "\n",
    "print(\"=================  Numbins = {}\".format(NUMBINS))\n",
    "\n",
    "def binfeatures(adf, feature, numbins, dup='drop'):\n",
    "    return (feature, pd.cut(adf[feature], numbins, retbins=True, duplicates=dup))\n",
    "    \n",
    "\n",
    "def dobinning(adf, numbins, columns):\n",
    "    # plt.figure(figsize=(16, 22)) \n",
    "    # plt.tight_layout()\n",
    "    # plt.axis('off')\n",
    "\n",
    "    for i,(feature,(res,bins)) in enumerate(map(lambda c: binfeatures(adf, c, numbins), columns)):\n",
    "        adf[feature] = res.astype('category')\n",
    "        print(\"Feature: \", feature)\n",
    "        print(\"Bins.  : \", bins)\n",
    "        # res.value_counts(sort=False).plot(kind='bar')\n",
    "        # ax = plt.subplot(7,4,i+1)\n",
    "        # ax.set_title(feature)\n",
    "        # ax.set_axis_off()\n",
    "\n",
    "    # Encode the Bins as numerical values, check the Distributions\n",
    "    print(\"\\n================== Encoding bins using OrdinalEncoder\")\n",
    "    cat_columns = adf.select_dtypes(['category']).columns\n",
    "    print(cat_columns)\n",
    "\n",
    "    # learn the categories, get encoders for each category\n",
    "    encs = list(map(lambda col: encodecategories(adf, col, preprocessing.OrdinalEncoder()), cat_columns))\n",
    "\n",
    "    # le = preprocessing.OrdinalEncoder()\n",
    "    # le.fit(adf[cat_columns])\n",
    "    # tle = le.transform(adf[cat_columns])\n",
    "    # adf[cat_columns] = tle\n",
    "\n",
    "    # df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # plt.show()\n",
    "    \n",
    "dobinning(df, NUMBINS, list(uniq(chain(gaussians,uniforms))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2efd4",
   "metadata": {
    "id": "8dc2efd4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea1ceb",
   "metadata": {
    "id": "e6ea1ceb"
   },
   "outputs": [],
   "source": [
    "# Check the Data types of All our columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886ea59",
   "metadata": {
    "id": "5886ea59"
   },
   "source": [
    "Have a look at Multimodal and Bimodal distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c62bf",
   "metadata": {
    "id": "840c62bf"
   },
   "outputs": [],
   "source": [
    "sdists = ['Att01', 'Att08', 'Att23', 'Att21']\n",
    "\n",
    "df[sdists].describe() # Maybe True false values ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DTO-RGAJh6AZ",
   "metadata": {
    "id": "DTO-RGAJh6AZ"
   },
   "source": [
    "Use Z-score scaling for each of the columns which is sensitive to outliers which may be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc36d6",
   "metadata": {
    "id": "14bc36d6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Z-Score Scaling as we aren't concerned about outliers.\n",
    "toscale = ['Att{}'.format(str(n).zfill(2)) for n in range(0,30) if n != 9 ]\n",
    "print(df.columns)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[toscale])\n",
    "df[toscale] = scaler.transform(df[toscale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bce508",
   "metadata": {
    "id": "59bce508",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check duplication\n",
    "df[df.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSP1GD0bjH6m",
   "metadata": {
    "id": "jSP1GD0bjH6m"
   },
   "source": [
    "Separate our labelled and unalabelled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N5YhLvESnc_q",
   "metadata": {
    "id": "N5YhLvESnc_q"
   },
   "outputs": [],
   "source": [
    "udf = df[1000:].copy(deep=True) # Unlabelled data\n",
    "ldf =  df[:1000].copy(deep=True) # Labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BKN227QJuUux",
   "metadata": {
    "id": "BKN227QJuUux"
   },
   "outputs": [],
   "source": [
    "ldf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boGB81WquXYk",
   "metadata": {
    "id": "boGB81WquXYk"
   },
   "outputs": [],
   "source": [
    "udf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d71b3",
   "metadata": {
    "id": "a20d71b3"
   },
   "source": [
    "For each of the above issues your report should:\n",
    "\n",
    "    * Describe the relevant issue in your own words and explain why it is important to address it. Your explanation must consider the classification task that you will undertake subsequently.\n",
    "    * Demonstrate clearly that such an issue exists in the data with suitable illustration/evidence.\n",
    "    * Clearly state and explain your choice of action to address such an issue.\n",
    "    * Demonstrate convincingly that your action has addressed the issue satisfactorily. Where applicable, you should provide references to support your arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hznm5GsRiHgI",
   "metadata": {
    "id": "hznm5GsRiHgI"
   },
   "source": [
    "View correlation between attributes using df.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47ecbe",
   "metadata": {
    "id": "fa47ecbe"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5caf9",
   "metadata": {
    "id": "53b5caf9"
   },
   "outputs": [],
   "source": [
    "# use seaborn to do the plot\n",
    "fig, ax = plt.subplots(figsize=(22,22))         # Sample figsize in inches\n",
    "\n",
    "corrs = df.corr()\n",
    "\n",
    "# Correlation for preprocessing\n",
    "sns.heatmap(corrs, annot=True, cmap=plt.cm.Reds, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd667626",
   "metadata": {
    "id": "fd667626"
   },
   "outputs": [],
   "source": [
    "# Drop the sparse dataset and any other attributes we may not need,\n",
    "# Return main features and target for feature selection\n",
    "\n",
    "def preparefeatureselection(adf, todrop, errors='ignore'):\n",
    "    pX = adf[ list(filter(lambda h: h not in todrop , ['Att{}'.format(str(n).zfill(2)) for n in range(0,30)] )) ]\n",
    "    print(\"================= Columns used for feature selection \")\n",
    "    print(pX.columns)\n",
    "    print(\"================= Shape of remaining data \")\n",
    "    print(pX.shape)\n",
    "    py = adf['class']\n",
    "    return pX, py\n",
    "\n",
    "oX, y = preparefeatureselection(ldf, sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff8fc7",
   "metadata": {
    "id": "13ff8fc7"
   },
   "source": [
    "###  Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8es-LQjMj015",
   "metadata": {
    "id": "8es-LQjMj015"
   },
   "source": [
    "Get the best independent features for our target attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545bbc1",
   "metadata": {
    "id": "b545bbc1"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "from sklearn.feature_selection import  SelectPercentile, f_classif\n",
    "\n",
    "print(\"Columns: \", oX.columns)\n",
    "print(\"====== Incoming shape:  \", oX.shape)\n",
    "\n",
    "# Cherry pick the 60th percentile by correlation based on f_classif (Numerical -> Categorical classification)\n",
    "pselector = SelectPercentile(f_classif, percentile=60)\n",
    "pselector.fit(oX, y)\n",
    "print(\"====== Column selected\")\n",
    "\n",
    "# Mask against selected columns to a new DataFrame\n",
    "corrstrong = list(map(lambda a : a[1] , filter(lambda z : z[0], zip(pselector.get_support(), oX.columns))))\n",
    "\n",
    "print(corrstrong)\n",
    "\n",
    "\n",
    "X_P = pselector.transform(oX) \n",
    "print(\"======= Outgoing shape: \", X_P.shape)\n",
    "\n",
    "\n",
    "X_new = pd.DataFrame(X_P, columns=corrstrong)\n",
    "\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e89d3",
   "metadata": {
    "id": "066e89d3"
   },
   "outputs": [],
   "source": [
    "# Check final dataframe features\n",
    "X_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f03b1",
   "metadata": {
    "id": "966f03b1"
   },
   "source": [
    "Check the ratio of imbalance in our target categeory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5911b7",
   "metadata": {
    "id": "6f5911b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# See the number of count in each class -> Noticed the class imbalance ratio is like: 2:3:5\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ex5mzDDfa0an",
   "metadata": {
    "id": "Ex5mzDDfa0an"
   },
   "outputs": [],
   "source": [
    "y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owwpVq5Ni6kz",
   "metadata": {
    "id": "owwpVq5Ni6kz"
   },
   "source": [
    "Given the features in our dataset, lets find the minimum number of features that can be used to represent it, reducing the dimensionality. Let us first just check this relationship. \n",
    "\n",
    "This plot is just to give us an idea first for the whole dataset the test and prediction data will only be transformed and the training set will be fitted and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f7ede",
   "metadata": {
    "id": "d50f7ede"
   },
   "outputs": [],
   "source": [
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "pca = PCA().fit(X_new)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab47ec5",
   "metadata": {
    "id": "cab47ec5"
   },
   "outputs": [],
   "source": [
    "# DO PCA on the training and testing data\n",
    "# https://stats.stackexchange.com/questions/55718/pca-and-the-train-test-split\n",
    "# https://stats.stackexchange.com/a/125328\n",
    "# https://stats.stackexchange.com/questions/125172/pca-on-train-and-test-datasets-should-i-run-one-pca-on-traintest-or-two-separa\n",
    "# Therefore fit the PCA on the training data and transform the testing data\n",
    "\n",
    "def fitpca(X, comp=13):\n",
    "    pca = PCA(n_components=comp)\n",
    "    Xout = pca.fit_transform(X)\n",
    "    return Xout,pca\n",
    "\n",
    "\n",
    "def transformpca(X, pca):\n",
    "    X_PCA = pca.transform(X)\n",
    "    print(\"original shape:   \", X.shape)\n",
    "    print(\"transformed shape:\", X_PCA.shape)\n",
    "    return X_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752ca8b",
   "metadata": {
    "id": "8752ca8b"
   },
   "source": [
    "#### Split our Training and Testing data and Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86be61",
   "metadata": {
    "id": "6f86be61"
   },
   "outputs": [],
   "source": [
    "# Remember we have a class imbalance, use stratify on train test split\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=2,\n",
    "                                                    stratify=y   # For the class imbalance\n",
    "                                                   )\n",
    "# Fit PCA on our training\n",
    "X_train, pca = fitpca(X_train, comp=13)\n",
    "\n",
    "# Only transfor the test\n",
    "X_test = transformpca(X_test, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb67159",
   "metadata": {
    "id": "8bb67159"
   },
   "source": [
    "# Data Classification\n",
    "\n",
    "For this task, you will demonstrate convincingly how you select, train, and fine tune your predictive\n",
    "models to predict the missing labels. You must use at least the three (3) classifiers that have been\n",
    "discussed in the workshops, namely k-NN, Naive Bayes, and Decision Trees. You can also select\n",
    "additional classifiers (both base classifiers and meta-classifiers). \n",
    "Attempt and report the following:\n",
    "\n",
    "• Class imbalance: the original labelled data is not equally distributed between the three classes.\n",
    "You need to demonstrate that such an issue exists within the data, explain the importance of\n",
    "this issue, and describe how you address this problem.\n",
    "\n",
    "• Model training and tuning: Every classifier typically has hyperparameters to tune in order. For\n",
    "each classifier, you need to select (at least one) and explain the tuning hyperparameters of your\n",
    "choice. You must select and describe a suitable cross-validation/validation scheme that can\n",
    "measure the performance of your model on labelled data well and can address the class\n",
    "imbalance issue. Then you will need to conduct the actual tuning of your model and report the\n",
    "tuning results in detail. You are expected to look at several classification performance metrics\n",
    "and make comments on the classification performance of each model. Finally, you will need to\n",
    "clearly indicate and justify the selected values of the tuning hyperparameters of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82731c5b",
   "metadata": {
    "id": "82731c5b"
   },
   "source": [
    "## KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01657633",
   "metadata": {
    "id": "01657633",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "print()\n",
    "print(\"==================== Check the Split ratios\")\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))\n",
    "print()\n",
    "\n",
    "parameters = {'weights': ('uniform', 'distance'),\n",
    "              'p': [1,2],\n",
    "              'metric': ['euclidean', 'manhattan','minkowski'],\n",
    "              'n_neighbors':[1 , 3, 5, 7, 11, 17, 21]}\n",
    "\n",
    "# https://stackoverflow.com/questions/34731421/whats-the-difference-between-kfold-and-shufflesplit-cv\n",
    "# https://xzz201920.medium.com/stratifiedkfold-v-s-kfold-v-s-stratifiedshufflesplit-ffcae5bfdf\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=True, random_state=2)\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.33, random_state=2)\n",
    "\n",
    "\n",
    "print(\"==================== Fitting KNN\")\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "k_gscv = GridSearchCV(estimator=knn,\n",
    "                    param_grid=parameters,\n",
    "                    cv= skf, # Cross validation\n",
    "                    scoring='balanced_accuracy')\n",
    "\n",
    "results = k_gscv.fit(X_train, y_train)\n",
    "\n",
    "print(\"==================== Crossvalidation results\")\n",
    "print(results.best_params_, results.best_score_)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
    "\n",
    "print(\"==================== Confusion Matrix\")\n",
    "metrics.plot_confusion_matrix(k_gscv,\n",
    "                              X_test, y_test,\n",
    "                              display_labels=['0', '1', '2'],\n",
    "                              ax=ax)\n",
    "\n",
    "print(\"==================== Trial prediction\")\n",
    "y_pred = k_gscv.predict(X_test)\n",
    "\n",
    "print(\"==================== Classification report\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score = \", sep='', end='')\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097948d",
   "metadata": {
    "id": "2097948d"
   },
   "source": [
    "## Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf580a8",
   "metadata": {
    "id": "0bf580a8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Use a descision tree classifier\n",
    "## Using a descision tree classifier:\n",
    "## - Train the classifier using both the `Gini index` and `entropy` criterion for splitting.\n",
    "## - Choose the classifier which has the highest F1 score as your best classifier.\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "parameters = {'criterion': ('gini', 'entropy'),\n",
    "              'min_samples_split':[3, 5, 15, 20, 25 ],\n",
    "              'min_samples_leaf': [n for n in range(2,11)],\n",
    "              'max_depth': [n for n in range(3,9)],\n",
    "              'class_weight': [{0.0:2, 1.0:3, 2.0:5}],\n",
    "              'splitter': ('best','random'),\n",
    "             }\n",
    "\n",
    "print(\"==================== Fitting Decision tree\")\n",
    "                                # Counter({2.0: 373, 1.0: 226, 0.0: 151})\n",
    "dtc = tree.DecisionTreeClassifier(random_state=2)\n",
    "d_gscv = GridSearchCV(estimator=dtc,\n",
    "                    param_grid=parameters,\n",
    "                    cv=skf,\n",
    "                    scoring='balanced_accuracy')\n",
    "\n",
    "results = d_gscv.fit(X_train, y_train)\n",
    "\n",
    "print(\"==================== Crossvalidation results\")\n",
    "print(results.best_params_, results.best_score_)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "# tree.plot_tree(\n",
    "#               gscv.best_estimator_, \n",
    "#               filled=True, # color the nodes based on class/purity\n",
    "#               ax=ax, fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
    "\n",
    "print(\"==================== Confusion matrix\")\n",
    "metrics.plot_confusion_matrix(d_gscv,\n",
    "                              X_test, y_test,\n",
    "                              display_labels=['0', '1', '2'],\n",
    "                              ax=ax)\n",
    "\n",
    "y_pred = d_gscv.predict(X_test)\n",
    "\n",
    "print(\"==================== Classification report\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy = \", sep='', end='')\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e742190",
   "metadata": {
    "id": "7e742190"
   },
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814b1aa",
   "metadata": {
    "id": "2814b1aa"
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# no real parameters to adjust here\n",
    "fig, ax = plt.subplots(1,1)\n",
    "nb = naive_bayes.GaussianNB()\n",
    "classifier = nb.fit(X_train, y_train)\n",
    "\n",
    "metrics.plot_confusion_matrix(classifier,\n",
    "                              X_test, y_test,\n",
    "                              display_labels=['0', '1', '2'],\n",
    "                              ax=ax)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy = \", sep='', end='')\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b4718",
   "metadata": {
    "id": "2b2b4718"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "Use the best two (2) models that you have identified in the previous step to predict the\n",
    "missing class labels of the last 200 samples in the original data set. Clearly explain in\n",
    "detail how you arrive at the prediction.\n",
    "o Provide your prediction in the report by creating a table, the first column is the sample\n",
    "ID, the second and third columns are the predicted class labels respectively. Observe\n",
    "and comment on the prediction that you have produced.\n",
    "o Produce an sqlite3 database file with the name Answers.sqlite that contains your\n",
    "prediction in the format: the first column is the sample ID, the second and third columns\n",
    "are the predicted class labels. All columns should be integers. This file must be\n",
    "submitted electronically with the electronic copy of the report via Blackboard. An\n",
    "example of such a file is given below:\n",
    "\n",
    "- Using accuracy and F1 score : KNN and Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144d3ed",
   "metadata": {
    "id": "2144d3ed"
   },
   "outputs": [],
   "source": [
    "# View the unlabelled entries for sparse data\n",
    "# Only the predicted elements seem to be missing which is OK\n",
    "udf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82651d4c",
   "metadata": {
    "id": "82651d4c"
   },
   "outputs": [],
   "source": [
    "udf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95daf1d",
   "metadata": {
    "id": "b95daf1d"
   },
   "outputs": [],
   "source": [
    "udf[udf.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a4999",
   "metadata": {
    "id": "5c6a4999"
   },
   "outputs": [],
   "source": [
    "uX, uy = preparefeatureselection(udf, sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193817e5",
   "metadata": {
    "id": "193817e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the features used on our models ( in the 60th percentile )\n",
    "# uX[corrstrong]\n",
    "fX = pd.DataFrame(pselector.transform(uX), columns=corrstrong)\n",
    "fX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320816f4",
   "metadata": {
    "id": "320816f4"
   },
   "outputs": [],
   "source": [
    "# Apply PCA on our prediction data\n",
    "preddata = transformpca(fX, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474bc6b",
   "metadata": {
    "id": "6474bc6b"
   },
   "source": [
    "## KNN Classifier prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9c5f3",
   "metadata": {
    "id": "9de9c5f3"
   },
   "outputs": [],
   "source": [
    "# KNN prediction based on features\n",
    "ky = k_gscv.predict(preddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d5bb5",
   "metadata": {
    "id": "7e8d5bb5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the Distribution of the prediction\n",
    "Counter(ky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_wRumJWAGwDB",
   "metadata": {
    "id": "_wRumJWAGwDB"
   },
   "outputs": [],
   "source": [
    "ky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122baa4d",
   "metadata": {
    "id": "122baa4d"
   },
   "source": [
    "## Naive Bayes prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a0fe8",
   "metadata": {
    "id": "161a0fe8"
   },
   "outputs": [],
   "source": [
    "# Naive bayes prediction based on features\n",
    "ny = nb.predict(preddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ba914",
   "metadata": {
    "id": "396ba914"
   },
   "outputs": [],
   "source": [
    "# Check the Distribution of the prediction\n",
    "Counter(ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308IUym2CDTA",
   "metadata": {
    "id": "308IUym2CDTA"
   },
   "outputs": [],
   "source": [
    "pprint(ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LbhV20sh1Utm",
   "metadata": {
    "id": "LbhV20sh1Utm"
   },
   "source": [
    "our prediction distribution relatively reflects the input data in terms of distribution. It is likely that the imbalance leaked through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ea1b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "921ea1b1",
    "outputId": "0d08d055-d7c7-4f18-dd7e-806413ecc6e7"
   },
   "outputs": [],
   "source": [
    "# Prepare our dataframe for exporting\n",
    "finaldf = pd.DataFrame([\n",
    "    range(1000,1200), ky, ny\n",
    "]).transpose()\n",
    "\n",
    "finaldf.columns = ['index', 'Predict1', 'Predict2']\n",
    "\n",
    "finaldf['index'] = finaldf['index'].astype(int)\n",
    "finaldf['Predict1'] = finaldf['Predict1'].astype(int)\n",
    "finaldf['Predict2'] = finaldf['Predict2'].astype(int)\n",
    "\n",
    "finaldf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8d9fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "d7c8d9fd",
    "outputId": "5bf06bcf-2c8e-403d-e77a-2dbc6640e8df"
   },
   "outputs": [],
   "source": [
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a276d8f",
   "metadata": {
    "id": "5a276d8f"
   },
   "outputs": [],
   "source": [
    "# Export the data\n",
    "ccon = sqlite3.connect(\"Answers.sqlite\")\n",
    "\n",
    "finaldf.to_sql(\"data\", ccon,  if_exists='replace', index=False) # dtype={\"Predict1\": Integer(), \"Predict2\": Integer() })"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_wakuloba_19998246.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
